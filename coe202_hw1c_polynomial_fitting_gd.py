# -*- coding: utf-8 -*-
"""CoE202_HW1c_polynomial_fitting_GD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dU2QgA55rOIyVxcUg17VH2MbgKVnHKrp

# [CoE202] **[Homework1c]** Polynomial regression

In this section, you are going to implement polynomial regression algorithms using gradient descent.

### 0. Importing packages

For this assignment we need Numpy and Matplotlib.
"""

# this is just an annotation
import numpy as np # this is for importing numpy library (and we will use abbreviation np for that)
import matplotlib.pyplot as plt # this is for importing matplotlib.pyplot (library for graph plot)

"""### 1. Introduction 
Given data points (x, y), we want to find non-linear estimator that fits well on the data. We are going to consider the only case when x and y are single dimension.
"""

# data points
X = np.array([0.0, 1.0, 2.0, 3.0,  4.0,  5.0]) 
y = np.array([0.0, 0.8, 2.9, 5.1, 8.8, 15.1])

# plot data
plt.plot(X, y, 'bo')

"""### 2. Polynomial regression with gradient descent


"""

def poly_features(X, K):
    """Compute the feature matrix Phi

    Arguments:
      X: input data of size N vector
      K: degree of the polynomial
    
    Returns:
      Phi: feature matrix of size N x (K + 1)
    """
    # [Problem 1] returns feature matrix Phi from input and degree of the polynomial
    # HINT: np.vander to generate a Vandermonde matrix.
    # Fill out here       
    Phi = Phi = np.vander(X, K+1, increasing=True)
    
    return Phi

def vectorize_y(y):
    y_vec = y.reshape(-1, 1) 
    return y_vec

K = 2
Phi = poly_features(X, K)
y_vec = vectorize_y(y)

print(Phi)
print(y_vec)

# polynomial fitting (2nd order polynomial fitting)
# linear fitting using gradient descent
def np_linear_regression_gd(Phi, y_vec):
    """Compute the coefficients by gradient descent polynomial fitting.

    Arguments:
      Phi: feature matrix of size N x (K + 1)
      y_vec: training targets of size N x 1
    
    Returns:
      theta = coefficients of the polynomial function.
    """
    alpha = 0.003             # learning rate
    num_iter = 5000           # number of iterations
    N, D = Phi.shape
    theta = np.zeros((D, 1))  # initialize theta
    
    # iteratively apply gradient descent
    for i in range(num_iter):
        # [Problem 2] Calculate gradient grad_L, from Phi, theta and y_vec
        # calculate gradients
        
        # Fill here        
        grad_L = 2/N * Phi.T @ ( Phi@theta - y_vec)

        # [Problem 3] Update theta by alpha and grad_L
        # update theta
        
        # Fill here        
        theta = theta - alpha*grad_L
  
    return theta

"""### 3. Testing algorithms."""

# get coefficients 
theta_ml = np_linear_regression_gd(Phi, y_vec)
print(theta_ml)

# test inputs
Xtest = np.linspace(-1,6,100)

# feature matrix for test inputs
Phi_test = poly_features(Xtest, K)

y_pred = Phi_test @ theta_ml # predicted y-values

plt.figure()
plt.plot(X, y, '+')
plt.plot(Xtest, y_pred)
plt.xlabel("$x$")
plt.ylabel("$y$");